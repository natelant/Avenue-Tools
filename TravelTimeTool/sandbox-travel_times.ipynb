{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from datetime import datetime\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "\n",
    "\n",
    "# Set the max_columns option to display all columns horizontally\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard code the inputs and read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = \"data/TOD/2100\"\n",
    "start_time_str = \"7:00\"\n",
    "stop_time_str = \"9:00\"\n",
    "start_time = datetime.strptime(start_time_str, \"%H:%M\").time()\n",
    "stop_time = datetime.strptime(stop_time_str, \"%H:%M\").time()\n",
    "filter_date = \"03/26/2024 00:00:00\"\n",
    "implementation_date = pd.to_datetime(filter_date)\n",
    "output_file = \"testing.xlsx\"\n",
    "start_date =  \"No\"#\"06/20/2023\"\n",
    "primary_movement = 'Pioneer EB.csv'\n",
    "category_order = [primary_movement]\n",
    "# 2100: dates_to_exclude = ['2024-03-20', '2024-03-21', '2024-03-22', '2024-03-25', '2024-03-26', '2024-03-27', '2024-03-28', '2024-03-29', '2024-04-01', '2024-04-02', '2024-04-03']\n",
    "# Foothill: dates_to_exclude = ['2024-03-04', '2024-03-05', '2024-03-06', '2024-03-07', '2024-03-08', '2024-03-14', '2024-03-15', '2024-03-18', '2024-03-19', '2024-03-20', '2024-03-21', '2024-03-22', '2024-03-25', '2024-03-26']\n",
    "dates_to_exclude = ['2024-11-04']\n",
    "output_summary_table = 'output/Redwood_AM_summarytable.html'\n",
    "\n",
    "\n",
    "# Read all CSV files from the specified folder\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine all CSV files into a single DataFrame\n",
    "dfs = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Add a new column with the source filename\n",
    "    df['source_file'] = file\n",
    "    dfs.append(df)\n",
    "\n",
    "    combined_data = pd.concat(dfs, ignore_index=True)\n",
    "    combined_data['local_datetime'] = pd.to_datetime(combined_data['local_datetime'])\n",
    "    #clean_data = combined_data[combined_data['local_datetime'] >= start_date]\n",
    "    \n",
    "    # Filters all data before the start date or skips this step if the answer was NO\n",
    "    if start_date.lower() not in [\"no\", \"n\"]:\n",
    "        # Filter combined_data based on the condition\n",
    "        clean_data = combined_data[combined_data['local_datetime'] >= start_date]\n",
    "    else:\n",
    "        # If start_date is \"no\", assign clean_data to combined_data\n",
    "        clean_data = combined_data\n",
    "\n",
    "    \n",
    "# filter out the filter dates here.\n",
    "# Filter out the data for the specified dates\n",
    "clean_data = clean_data[~clean_data['local_datetime'].dt.normalize().isin(dates_to_exclude)]    \n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column 'before_after' based on the input implementation date\n",
    "clean_data.loc[:,'before_after'] = np.where(pd.to_datetime(clean_data['local_datetime']) < filter_date, 'before', 'after')\n",
    "\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an hour variable to compare travel times within the hour to an hourly average\n",
    "clean_data.loc[:,'hour'] = clean_data['local_datetime'].dt.hour\n",
    "clean_data.loc[:,'time'] = clean_data['local_datetime'].dt.time\n",
    "clean_data.loc[:,'day'] = clean_data['local_datetime'].dt.date\n",
    "clean_data.loc[:,'day_of_week'] = clean_data['local_datetime'].dt.day_of_week\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'time' column and calculate the average travel time for each time bin\n",
    "time_bin_avg = clean_data.groupby('time')['avg_travel_time'].mean().reset_index()\n",
    "\n",
    "# Merge the average travel time back to the original DataFrame based on the 'time' column\n",
    "merged_data = pd.merge(clean_data, time_bin_avg, on='time', suffixes=('', '_avg'))\n",
    "\n",
    "# Rename the new column containing the average travel time\n",
    "merged_data.rename(columns={'avg_travel_time_avg': 'time_bin_avg'}, inplace=True)\n",
    "merged_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the z-score for each travel time based on the average for its respective hour\n",
    "merged_data['z_score'] = (merged_data['avg_travel_time'] - merged_data['time_bin_avg']) / merged_data['time_bin_avg'].std()\n",
    "# Set a threshold for outliers (e.g., z-score greater than 3 or less than -3)\n",
    "outlier_threshold = 3\n",
    "outliers = merged_data[abs(merged_data['z_score']) > outlier_threshold]\n",
    "\n",
    "# Filter data based on peak hour range and remove outliers\n",
    "peak_hour_data = merged_data[(merged_data.local_datetime.dt.time >= start_time)&(merged_data.local_datetime.dt.time <= stop_time)]\n",
    "filtered_data = peak_hour_data[abs(peak_hour_data.z_score) < outlier_threshold]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Travel Times ---------------------------------\n",
    "# Create summary table to compare before and after travel times, excluding the outliers\n",
    "summary_table = (\n",
    "    filtered_data\n",
    "    .groupby(['source_file','before_after'])\n",
    "    ['avg_travel_time']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index='source_file', columns='before_after', values='avg_travel_time')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Reorder the columns\n",
    "# Check if 'before' and 'after' columns exist in the DataFrame\n",
    "if 'before' in summary_table.columns and 'after' in summary_table.columns:\n",
    "    # Reorder the columns if both 'before' and 'after' exist\n",
    "    summary_table = summary_table[['source_file', 'before', 'after']]\n",
    "    # add the difference column\n",
    "    summary_table['Difference (sec)'] = (summary_table['after'] - summary_table['before']) * 60\n",
    "elif 'before' in summary_table.columns:\n",
    "    # Insert NA values for 'after' and reorder the columns\n",
    "    summary_table['after'] = np.nan\n",
    "    summary_table = summary_table[['source_file', 'before', 'after']]\n",
    "elif 'after' in summary_table.columns:\n",
    "    # Insert NA values for 'before' and reorder the columns\n",
    "    summary_table['before'] = np.nan\n",
    "    summary_table = summary_table[['source_file', 'before', 'after']]\n",
    "\n",
    "html_summary_table = summary_table.to_html()\n",
    "# Write HTML content to a file\n",
    "with open(output_summary_table, 'w') as file:\n",
    "    file.write(html_summary_table)\n",
    "\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store variables\n",
    "avg_before_1 = summary_table.loc[0, 'before']\n",
    "avg_before_2 = summary_table.loc[1, 'before']\n",
    "\n",
    "avg_after_1 = summary_table.loc[0, 'after']\n",
    "avg_after_2 = summary_table.loc[1, 'after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Time Series (Unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly for time series, wrapped  by route\n",
    "# Create a Plotly figure\n",
    "fig_overall = px.line(clean_data, x='local_datetime', y='avg_travel_time', color='source_file',  title='Average Travel Time Over Time')\n",
    "\n",
    "# Show the chart\n",
    "fig_overall.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned Time Series - showing before and after comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean time series plot with horizontal lines ## NOTE maybe this one needs to be a ---- DAILY AVERAGE ----\n",
    "\n",
    "# start with filtered data then group by day and route and before_after then mean. \n",
    "daily_avg_data = filtered_data.groupby(['source_file','day'])['avg_travel_time'].mean().reset_index().sort_values(by='day')\n",
    "daily_avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category rules: keep it alphabetical... just go with the default flow\n",
    "\n",
    "fig_daily_average = px.line(daily_avg_data, x='day', y='avg_travel_time', color='source_file', facet_row='source_file', title='Daily Average Travel Time Over Time')\n",
    "\n",
    "# add horizontal lines\n",
    "\n",
    "# create data for horizontal line length extents\n",
    "before_data = filtered_data[(filtered_data['before_after'] == 'before')]\n",
    "after_data = filtered_data[(filtered_data['before_after'] == 'after')]\n",
    "\n",
    "# add before lines, stops at implementation date ## NOTE: it is weird that the rows seem to be backwards... 1 from summary table matches 2 in the facet but shows up first on the plot...\n",
    "fig_daily_average.add_trace(go.Scatter(x=before_data['local_datetime'], y=[avg_before_1] * len(filtered_data),\n",
    "                    mode='lines', name='Average Before Implementation', line=dict(color='red', dash='dash')), row=2, col=1)\n",
    "fig_daily_average.add_trace(go.Scatter(x=before_data['local_datetime'], y=[avg_before_2] * len(filtered_data),\n",
    "                    mode='lines', name='Average Before Implementation', line=dict(color='red', dash='dash')), row=1, col=1)\n",
    "\n",
    "# add after lines, continuous for comparison\n",
    "fig_daily_average.add_trace(go.Scatter(x=filtered_data['local_datetime'], y=[avg_after_1] * len(filtered_data),\n",
    "                    mode='lines', name='Average After Implementation', line=dict(color='green', dash='dash')), row=2, col=1)\n",
    "fig_daily_average.add_trace(go.Scatter(x=filtered_data['local_datetime'], y=[avg_after_2] * len(filtered_data),\n",
    "                    mode='lines', name='Average After Implementation', line=dict(color='green', dash='dash')), row=1, col=1)\n",
    "\n",
    "# add vertical line showing implementation date\n",
    "fig_daily_average.add_trace(go.Scatter(x=[implementation_date, implementation_date], y=[filtered_data['avg_travel_time'].min(), filtered_data['avg_travel_time'].max()],\n",
    "                    mode='lines', name='Implementation Date', line=dict(color='black', dash='solid')), row=1, col=1)\n",
    "fig_daily_average.add_trace(go.Scatter(x=[implementation_date, implementation_date], y=[filtered_data['avg_travel_time'].min(), filtered_data['avg_travel_time'].max()],\n",
    "                    mode='lines', name='Implementation Date', line=dict(color='black', dash='solid')), row=2, col=1)\n",
    "\n",
    "fig_daily_average.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly for time of day, wrapped by route\n",
    "time_of_day_data = filtered_data.groupby(['source_file','before_after','time'])['avg_travel_time'].mean().reset_index()\n",
    "\n",
    "fig_time_of_day = px.line(time_of_day_data, x='time', y='avg_travel_time', color='before_after', facet_row='source_file', title='Average Travel Time Over Time-of-Day')\n",
    "\n",
    "# add horizontal lines ## NOTE: it is weird that the rows seem to be backwards... 1 from summary table matches 2 in the facet but shows up first on the plot...\n",
    "# before\n",
    "fig_time_of_day.add_trace(go.Scatter(x=time_of_day_data['time'], y=[avg_before_1] * len(filtered_data),\n",
    "                    mode='lines', name='Average Before Implementation', line=dict(color='red', dash='dash')), row=2, col=1)\n",
    "fig_time_of_day.add_trace(go.Scatter(x=time_of_day_data['time'], y=[avg_before_2] * len(filtered_data),\n",
    "                    mode='lines', name='Average Before Implementation', line=dict(color='red', dash='dash')), row=1, col=1)\n",
    "#after\n",
    "fig_time_of_day.add_trace(go.Scatter(x=time_of_day_data['time'], y=[avg_after_1] * len(filtered_data),\n",
    "                    mode='lines', name='Average After Implementation', line=dict(color='blue', dash='dash')), row=2, col=1)\n",
    "fig_time_of_day.add_trace(go.Scatter(x=time_of_day_data['time'], y=[avg_after_2] * len(filtered_data),\n",
    "                    mode='lines', name='Average After Implementation', line=dict(color='blue', dash='dash')), row=1, col=1)\n",
    "\n",
    "\n",
    "fig_time_of_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostics. Shows histogram distribution. Travel time bins in x axis and frequency in the y axis\n",
    "# why is this backwards (red is \"after\" now...)\n",
    "\n",
    "fig_dist = px.histogram(filtered_data, x='avg_travel_time', color='before_after', barmode='overlay', facet_row='source_file', title='Distribution of Travel Times')\n",
    "fig_dist.update_traces(opacity=0.9)\n",
    "\n",
    "fig_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write plot to HTML\n",
    "# Save the chart as an HTML file\n",
    "output_plot_file_path = \"output/Foothill_PM_distribution.html\"\n",
    "plot(fig_dist, filename=output_plot_file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we know the difference is significant?\n",
    "# how do we monitor the change overtime (APIs, email report updates)\n",
    "# or PLOTLY website\n",
    "# I need API to clearguide and ATSPM aggregates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
